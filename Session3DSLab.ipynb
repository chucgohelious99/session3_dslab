{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session3DSLab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIoK6e9nEAUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf2\n",
        "import numpy as np\n",
        "import random \n",
        "\n",
        "\n",
        "class DataReader:\n",
        "  def __init__(self, data_path, batch_size, vocab_size):\n",
        "    self._batch_size= batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines= f.read().splitlines()\n",
        "    \n",
        "    self._data=[]\n",
        "    self._labels=[]\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      tfidf_vec= [0.0 for i in range(vocab_size)]\n",
        "      feature= line.split('<fff>')\n",
        "      label, doc_id= int(feature[0]), int(feature[1])\n",
        "      tokens= feature[2].split()\n",
        "      for token in tokens:\n",
        "        ind, value= int(token.split(\":\")[0]), \\\n",
        "                    float(token.split(\":\")[1])\n",
        "        tfidf_vec[ind]= value\n",
        "      \n",
        "      self._data.append(tfidf_vec)\n",
        "      self._labels.append(label)\n",
        "\n",
        "    self._data= np.array(self._data)\n",
        "    self._labels= np.array(self._labels)\n",
        "\n",
        "    self._num_epoch=0\n",
        "    self._batch_id=0\n",
        "\n",
        "  def next_batch(self):\n",
        "    #lấy data từ trong batch tiếp theo và xáo chộn lên\n",
        "    start= self._batch_id * self._batch_size\n",
        "    end= start + self._batch_size\n",
        "    self._batch_id +=1\n",
        "\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      # khi đến batch cuối, một epoch có kích thước bằng data, nên \n",
        "      # khi chọn data set cho từng patch cần xáo chộn ngẫu nhiên rồi chọn cho đủ patch\n",
        "      # dù cho trong 1 epoch có thể không chứa toàn bộ dữ liệu nhưng model có thể học dần \n",
        "      #dần thông qua các epoch\n",
        "      end= len(self._data)\n",
        "      self._num_epoch+=1 # chuyển sang epoch mới\n",
        "      self._batch_id=0\n",
        "      \n",
        "      # xáo trộn data\n",
        "      indices= list(range(len(self._data)))\n",
        "      random.seed(2020)\n",
        "      random.shuffle(indices)\n",
        "      self._data, self._labels= self._data[indices], self._labels[indices]\n",
        "\n",
        "    return self._data[start:end], self._labels[start:end]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-W2vL1dLFGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "tf2.disable_eager_execution()\n",
        "\n",
        "NUM_CLASSES= 20 #8\n",
        "#Triển khai MLP\n",
        "class MLP:\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    self._vocab_size= vocab_size\n",
        "    self._hidden_size= hidden_size\n",
        "\n",
        "  def build_graph(self):\n",
        "    self._x= tf2.placeholder(tf2.float32, shape=[None, self._vocab_size])\n",
        "    self._real_y= tf2.placeholder(tf2.int32, shape=[None,])\n",
        "\n",
        "    # xây dựng biến chứa trọng số và bias cho các lớp\n",
        "    with tf2.variable_scope(\"other_charge\", reuse=tf2.AUTO_REUSE) as scope:\n",
        "      weight_1= tf2.get_variable(\n",
        "          name='weight_input_hidden',\n",
        "          shape=(self._vocab_size,self._hidden_size),\n",
        "          initializer= tf2.random_normal_initializer(seed=2020),\n",
        "          # reuse= True\n",
        "      )\n",
        "      biases_1= tf2.get_variable(\n",
        "          name='biases_input_hidden',\n",
        "          shape=(self._hidden_size),\n",
        "          initializer= tf2.random_normal_initializer(seed=2020),\n",
        "          # reuse= True\n",
        "      )\n",
        "\n",
        "      weight_2= tf2.get_variable(\n",
        "          name='weight_output_hidden',\n",
        "          shape=(self._hidden_size, NUM_CLASSES),\n",
        "          initializer= tf2.random_normal_initializer(seed=2020),\n",
        "          # reuse= True\n",
        "      )\n",
        "      biases_2= tf2.get_variable(\n",
        "          name='biases_output_hidden',\n",
        "          shape=(NUM_CLASSES),\n",
        "          initializer= tf2.random_normal_initializer(seed=2020),\n",
        "          # reuse= True\n",
        "      )\n",
        "\n",
        "      # xây dựng mô hình computation graph\n",
        "      hidden= tf.matmul(self._x, weight_1) + biases_1\n",
        "      hidden= tf.sigmoid(hidden)\n",
        "      logits= tf.matmul(hidden, weight_2)+ biases_2\n",
        "\n",
        "      labels_one_hot= tf.one_hot(indices=self._real_y, depth= NUM_CLASSES,\n",
        "                                dtype= tf.float32)\n",
        "      loss= tf.nn.softmax_cross_entropy_with_logits(labels= labels_one_hot,\n",
        "                                                    logits= logits)\n",
        "      loss= tf.reduce_mean(loss)\n",
        "\n",
        "      probs= tf.nn.softmax(logits)\n",
        "      predicted_labels= tf.argmax(probs, axis=1)\n",
        "      predicted_labels= tf.squeeze(predicted_labels)\n",
        "\n",
        "    return predicted_labels, loss\n",
        "\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    with tf2.variable_scope(\"other_charge\", reuse=tf2.AUTO_REUSE) as scope:\n",
        "      train_op= tf2.train.AdamOptimizer(learning_rate= learning_rate).minimize(loss)\n",
        "    return train_op\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvAp8tGwNStF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8852b812-ad72-44b1-d297-75d30873554e"
      },
      "source": [
        "# with open('/content/drive/My Drive/Python code/DS_Lab/words-idfs.txt') as f:\n",
        "with open('/content/drive/My Drive/Python code/DS_Lab2/Data/words_idfs.txt') as f:\n",
        "  vocab_size= len(f.read().splitlines())\n",
        "\n",
        "def load_dataset():\n",
        "  # hàm gọi đối tượng đọc dữ liệu\n",
        "  train_data_reader= DataReader(\n",
        "      # data_path='/content/drive/My Drive/Python code/DS_Lab/train_tf_idf.txt',\n",
        "      data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/train_tf_idf.txt',\n",
        "      batch_size= 50,\n",
        "      vocab_size= vocab_size\n",
        "  )\n",
        "  test_data_reader= DataReader(\n",
        "      # data_path='/content/drive/My Drive/Python code/DS_Lab/test_tf_idf.txt',\n",
        "      data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/test_tf_idf.txt',\n",
        "      batch_size= 50,\n",
        "      vocab_size= vocab_size\n",
        "  )\n",
        "  return train_data_reader, test_data_reader\n",
        "\n",
        "def save_parameter(name, value, epoch):\n",
        "  # hàm hỗ trợ lưu lại giá trị các tham số trong quá trình train\n",
        "  filename= name.replace(\":\",\"-colon-\")+ '-epoch-{}.txt'.format(epoch)\n",
        "  if len(value.shape)==1: # là 1 list\n",
        "    string_form= ','.join([str(number) for number in value])\n",
        "  else:# la mot ma tran --> viet thanh nhieu hang\n",
        "    string_form= '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
        "  with open('/content/drive/My Drive/Python code/DS_Lab/Session 3 DS Lab/'+ filename,'w') as  f:\n",
        "  # with open('/content/drive/My Drive/'+ filename,'w') as  f:\n",
        "    f.write(string_form)\n",
        "\n",
        "# thực thi mô hình coputation graph\n",
        "mlp= MLP(\n",
        "    vocab_size= vocab_size,\n",
        "    hidden_size=50\n",
        ")\n",
        "predicted_labels, loss= mlp.build_graph()\n",
        "train_op= mlp.trainer(loss= loss, learning_rate= 0.03)\n",
        "\n",
        "train_data_reader, test_data_reader= load_dataset()\n",
        "# mở một phiên chạy đồ thị\n",
        "with tf2.Session() as sess:\n",
        "  # train_data_reader, test_data_reader= load_dataset()\n",
        "  step=0\n",
        "  MAX_STEP= 1000\n",
        "  \n",
        "  steps=[]\n",
        "  losses=[]\n",
        "  sess.run(tf2.global_variables_initializer())\n",
        "  while step < MAX_STEP:\n",
        "    train_data, label_data= train_data_reader.next_batch()\n",
        "    predicted_label_eval, loss_eval, _ = sess.run(\n",
        "        [predicted_labels,loss, train_op],\n",
        "        feed_dict={\n",
        "            mlp._x: train_data,\n",
        "            mlp._real_y: label_data\n",
        "        }\n",
        "    )\n",
        "    step+=1\n",
        "    steps.append(step)\n",
        "    losses.append(loss_eval)\n",
        "    print(\"step: {}, loss: {}\".format(step, loss_eval))\n",
        "\n",
        "    if loss_eval < 1e-5:\n",
        "      break\n",
        "    # trainable_variable= tf2.trainable_variables()\n",
        "    # for variable in trainable_variable:\n",
        "    #   save_parameter(\n",
        "    #       name= variable.name,\n",
        "    #       value= variable.eval(),\n",
        "    #       epoch=train_data_reader._num_epoch\n",
        "    #   )\n",
        "  # lưu các tham số của mô hình trong quá trình training\n",
        "  trainable_variable= tf2.trainable_variables()\n",
        "  for variable in trainable_variable:\n",
        "    save_parameter(\n",
        "        name= variable.name,\n",
        "        value= variable.eval(),\n",
        "        epoch=train_data_reader._num_epoch\n",
        "    )\n",
        "  \n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(steps, losses, 'g')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 1, loss: 12.611571311950684\n",
            "step: 2, loss: 9.164813041687012\n",
            "step: 3, loss: 5.419309139251709\n",
            "step: 4, loss: 1.963434100151062\n",
            "step: 5, loss: 0.5800004601478577\n",
            "step: 6, loss: 0.061471737921237946\n",
            "step: 7, loss: 0.012930989265441895\n",
            "step: 8, loss: 0.002346897730603814\n",
            "step: 9, loss: 0.0002396911004325375\n",
            "step: 10, loss: 5.840027332305908\n",
            "step: 11, loss: 14.664942741394043\n",
            "step: 12, loss: 14.024436950683594\n",
            "step: 13, loss: 12.672224044799805\n",
            "step: 14, loss: 10.911805152893066\n",
            "step: 15, loss: 9.10333251953125\n",
            "step: 16, loss: 7.419517993927002\n",
            "step: 17, loss: 5.028095245361328\n",
            "step: 18, loss: 2.8794009685516357\n",
            "step: 19, loss: 1.201609492301941\n",
            "step: 20, loss: 0.27794480323791504\n",
            "step: 21, loss: 0.04956164211034775\n",
            "step: 22, loss: 14.701445579528809\n",
            "step: 23, loss: 20.409074783325195\n",
            "step: 24, loss: 19.736095428466797\n",
            "step: 25, loss: 19.06585693359375\n",
            "step: 26, loss: 17.9831485748291\n",
            "step: 27, loss: 16.06134796142578\n",
            "step: 28, loss: 14.140690803527832\n",
            "step: 29, loss: 12.162773132324219\n",
            "step: 30, loss: 10.390832901000977\n",
            "step: 31, loss: 8.310486793518066\n",
            "step: 32, loss: 5.707192420959473\n",
            "step: 33, loss: 2.703906774520874\n",
            "step: 34, loss: 11.189937591552734\n",
            "step: 35, loss: 12.320342063903809\n",
            "step: 36, loss: 11.723865509033203\n",
            "step: 37, loss: 11.209290504455566\n",
            "step: 38, loss: 10.101815223693848\n",
            "step: 39, loss: 8.537590026855469\n",
            "step: 40, loss: 7.570720195770264\n",
            "step: 41, loss: 5.917225360870361\n",
            "step: 42, loss: 4.528645038604736\n",
            "step: 43, loss: 2.796562433242798\n",
            "step: 44, loss: 1.2638667821884155\n",
            "step: 45, loss: 1.8653939962387085\n",
            "step: 46, loss: 14.126809120178223\n",
            "step: 47, loss: 13.910791397094727\n",
            "step: 48, loss: 12.959094047546387\n",
            "step: 49, loss: 12.080387115478516\n",
            "step: 50, loss: 11.10968017578125\n",
            "step: 51, loss: 9.334493637084961\n",
            "step: 52, loss: 7.559004306793213\n",
            "step: 53, loss: 5.947616100311279\n",
            "step: 54, loss: 3.9056825637817383\n",
            "step: 55, loss: 2.4663503170013428\n",
            "step: 56, loss: 0.9701554775238037\n",
            "step: 57, loss: 4.498513698577881\n",
            "step: 58, loss: 7.848628044128418\n",
            "step: 59, loss: 7.853790283203125\n",
            "step: 60, loss: 7.264181137084961\n",
            "step: 61, loss: 6.709742546081543\n",
            "step: 62, loss: 5.311686992645264\n",
            "step: 63, loss: 4.0785064697265625\n",
            "step: 64, loss: 2.6859519481658936\n",
            "step: 65, loss: 1.5243452787399292\n",
            "step: 66, loss: 0.7272409796714783\n",
            "step: 67, loss: 0.2089257389307022\n",
            "step: 68, loss: 0.08289201557636261\n",
            "step: 69, loss: 8.025897026062012\n",
            "step: 70, loss: 11.318703651428223\n",
            "step: 71, loss: 10.785490989685059\n",
            "step: 72, loss: 10.254663467407227\n",
            "step: 73, loss: 9.331791877746582\n",
            "step: 74, loss: 8.04786491394043\n",
            "step: 75, loss: 6.888031482696533\n",
            "step: 76, loss: 5.526533603668213\n",
            "step: 77, loss: 4.003261089324951\n",
            "step: 78, loss: 2.5890207290649414\n",
            "step: 79, loss: 1.4722175598144531\n",
            "step: 80, loss: 0.6444560885429382\n",
            "step: 81, loss: 11.635370254516602\n",
            "step: 82, loss: 11.6864013671875\n",
            "step: 83, loss: 11.144030570983887\n",
            "step: 84, loss: 10.51959228515625\n",
            "step: 85, loss: 9.592164039611816\n",
            "step: 86, loss: 8.739720344543457\n",
            "step: 87, loss: 7.232667446136475\n",
            "step: 88, loss: 5.852613925933838\n",
            "step: 89, loss: 4.002392292022705\n",
            "step: 90, loss: 2.873220443725586\n",
            "step: 91, loss: 1.635722041130066\n",
            "step: 92, loss: 1.6444705724716187\n",
            "step: 93, loss: 10.615522384643555\n",
            "step: 94, loss: 10.178523063659668\n",
            "step: 95, loss: 9.715927124023438\n",
            "step: 96, loss: 8.92591381072998\n",
            "step: 97, loss: 8.323173522949219\n",
            "step: 98, loss: 7.1067657470703125\n",
            "step: 99, loss: 6.181924343109131\n",
            "step: 100, loss: 5.004077911376953\n",
            "step: 101, loss: 4.073044776916504\n",
            "step: 102, loss: 2.7109622955322266\n",
            "step: 103, loss: 1.8265832662582397\n",
            "step: 104, loss: 2.3738012313842773\n",
            "step: 105, loss: 10.503246307373047\n",
            "step: 106, loss: 10.018214225769043\n",
            "step: 107, loss: 9.121590614318848\n",
            "step: 108, loss: 8.696514129638672\n",
            "step: 109, loss: 7.614997386932373\n",
            "step: 110, loss: 6.908011436462402\n",
            "step: 111, loss: 5.661653518676758\n",
            "step: 112, loss: 4.935232162475586\n",
            "step: 113, loss: 4.502578258514404\n",
            "step: 114, loss: 3.2423715591430664\n",
            "step: 115, loss: 2.7685914039611816\n",
            "step: 116, loss: 3.4568610191345215\n",
            "step: 117, loss: 9.619453430175781\n",
            "step: 118, loss: 8.656820297241211\n",
            "step: 119, loss: 8.112268447875977\n",
            "step: 120, loss: 7.698920726776123\n",
            "step: 121, loss: 7.527867317199707\n",
            "step: 122, loss: 7.057816028594971\n",
            "step: 123, loss: 6.337303638458252\n",
            "step: 124, loss: 5.978799819946289\n",
            "step: 125, loss: 5.740073204040527\n",
            "step: 126, loss: 5.257784366607666\n",
            "step: 127, loss: 4.930364608764648\n",
            "step: 128, loss: 6.07766580581665\n",
            "step: 129, loss: 13.31436538696289\n",
            "step: 130, loss: 12.767416954040527\n",
            "step: 131, loss: 12.693994522094727\n",
            "step: 132, loss: 11.104387283325195\n",
            "step: 133, loss: 10.780287742614746\n",
            "step: 134, loss: 9.71509075164795\n",
            "step: 135, loss: 8.958626747131348\n",
            "step: 136, loss: 8.758797645568848\n",
            "step: 137, loss: 8.036195755004883\n",
            "step: 138, loss: 7.584666728973389\n",
            "step: 139, loss: 7.234065055847168\n",
            "step: 140, loss: 6.127108097076416\n",
            "step: 141, loss: 3.6620850563049316\n",
            "step: 142, loss: 3.6827356815338135\n",
            "step: 143, loss: 3.4827651977539062\n",
            "step: 144, loss: 3.3550634384155273\n",
            "step: 145, loss: 3.202615737915039\n",
            "step: 146, loss: 3.0485053062438965\n",
            "step: 147, loss: 2.8669538497924805\n",
            "step: 148, loss: 2.712876558303833\n",
            "step: 149, loss: 2.5875372886657715\n",
            "step: 150, loss: 2.3239262104034424\n",
            "step: 151, loss: 2.2028989791870117\n",
            "step: 152, loss: 7.018496513366699\n",
            "step: 153, loss: 11.570036888122559\n",
            "step: 154, loss: 10.16451644897461\n",
            "step: 155, loss: 11.202159881591797\n",
            "step: 156, loss: 10.312822341918945\n",
            "step: 157, loss: 9.740506172180176\n",
            "step: 158, loss: 9.729621887207031\n",
            "step: 159, loss: 9.37014389038086\n",
            "step: 160, loss: 8.53804874420166\n",
            "step: 161, loss: 8.220907211303711\n",
            "step: 162, loss: 7.756875038146973\n",
            "step: 163, loss: 7.801188945770264\n",
            "step: 164, loss: 5.999380588531494\n",
            "step: 165, loss: 5.317142963409424\n",
            "step: 166, loss: 4.9478912353515625\n",
            "step: 167, loss: 4.997978687286377\n",
            "step: 168, loss: 4.773163795471191\n",
            "step: 169, loss: 4.670390605926514\n",
            "step: 170, loss: 4.325620651245117\n",
            "step: 171, loss: 4.261771202087402\n",
            "step: 172, loss: 4.07955265045166\n",
            "step: 173, loss: 3.8816025257110596\n",
            "step: 174, loss: 3.8233842849731445\n",
            "step: 175, loss: 3.584207773208618\n",
            "step: 176, loss: 7.211429595947266\n",
            "step: 177, loss: 8.128093719482422\n",
            "step: 178, loss: 8.11553955078125\n",
            "step: 179, loss: 7.988686561584473\n",
            "step: 180, loss: 7.835208892822266\n",
            "step: 181, loss: 7.587058067321777\n",
            "step: 182, loss: 7.396896839141846\n",
            "step: 183, loss: 7.270113468170166\n",
            "step: 184, loss: 7.130921840667725\n",
            "step: 185, loss: 6.864168643951416\n",
            "step: 186, loss: 6.764889717102051\n",
            "step: 187, loss: 6.6539506912231445\n",
            "step: 188, loss: 6.892336368560791\n",
            "step: 189, loss: 6.906254291534424\n",
            "step: 190, loss: 6.838945388793945\n",
            "step: 191, loss: 6.71340799331665\n",
            "step: 192, loss: 6.652278900146484\n",
            "step: 193, loss: 6.498123645782471\n",
            "step: 194, loss: 6.567513942718506\n",
            "step: 195, loss: 6.297549247741699\n",
            "step: 196, loss: 6.152148246765137\n",
            "step: 197, loss: 6.043822765350342\n",
            "step: 198, loss: 5.845637798309326\n",
            "step: 199, loss: 3.717446804046631\n",
            "step: 200, loss: 3.2912392616271973\n",
            "step: 201, loss: 3.228044033050537\n",
            "step: 202, loss: 3.1539528369903564\n",
            "step: 203, loss: 3.0659995079040527\n",
            "step: 204, loss: 2.930116891860962\n",
            "step: 205, loss: 2.861283540725708\n",
            "step: 206, loss: 2.7912673950195312\n",
            "step: 207, loss: 2.6313889026641846\n",
            "step: 208, loss: 2.434627056121826\n",
            "step: 209, loss: 2.3015973567962646\n",
            "step: 210, loss: 4.931568145751953\n",
            "step: 211, loss: 7.043295860290527\n",
            "step: 212, loss: 7.021122932434082\n",
            "step: 213, loss: 6.627048969268799\n",
            "step: 214, loss: 6.951876640319824\n",
            "step: 215, loss: 6.439516544342041\n",
            "step: 216, loss: 6.363767147064209\n",
            "step: 217, loss: 6.2330732345581055\n",
            "step: 218, loss: 6.232767105102539\n",
            "step: 219, loss: 6.920129776000977\n",
            "step: 220, loss: 9.21812629699707\n",
            "step: 221, loss: 9.136274337768555\n",
            "step: 222, loss: 8.999451637268066\n",
            "step: 223, loss: 8.948465347290039\n",
            "step: 224, loss: 8.723288536071777\n",
            "step: 225, loss: 8.614137649536133\n",
            "step: 226, loss: 3.2270548343658447\n",
            "step: 227, loss: 3.4031598567962646\n",
            "step: 228, loss: 3.7372124195098877\n",
            "step: 229, loss: 2.9609899520874023\n",
            "step: 230, loss: 3.5555341243743896\n",
            "step: 231, loss: 3.090926170349121\n",
            "step: 232, loss: 3.0008978843688965\n",
            "step: 233, loss: 3.5149075984954834\n",
            "step: 234, loss: 2.937966823577881\n",
            "step: 235, loss: 3.378676652908325\n",
            "step: 236, loss: 3.762375593185425\n",
            "step: 237, loss: 3.2655937671661377\n",
            "step: 238, loss: 3.437549352645874\n",
            "step: 239, loss: 2.9080591201782227\n",
            "step: 240, loss: 3.1846423149108887\n",
            "step: 241, loss: 2.7704758644104004\n",
            "step: 242, loss: 3.0170507431030273\n",
            "step: 243, loss: 2.9051196575164795\n",
            "step: 244, loss: 3.071185350418091\n",
            "step: 245, loss: 2.945096969604492\n",
            "step: 246, loss: 3.026611089706421\n",
            "step: 247, loss: 2.8708128929138184\n",
            "step: 248, loss: 3.22900390625\n",
            "step: 249, loss: 3.056258201599121\n",
            "step: 250, loss: 2.9435315132141113\n",
            "step: 251, loss: 2.7296271324157715\n",
            "step: 252, loss: 2.9227185249328613\n",
            "step: 253, loss: 2.6083474159240723\n",
            "step: 254, loss: 2.9321389198303223\n",
            "step: 255, loss: 2.697519540786743\n",
            "step: 256, loss: 2.690483808517456\n",
            "step: 257, loss: 2.5762979984283447\n",
            "step: 258, loss: 2.701695203781128\n",
            "step: 259, loss: 2.534571647644043\n",
            "step: 260, loss: 2.789177894592285\n",
            "step: 261, loss: 2.6523396968841553\n",
            "step: 262, loss: 2.6820623874664307\n",
            "step: 263, loss: 2.497293710708618\n",
            "step: 264, loss: 2.1422038078308105\n",
            "step: 265, loss: 2.3151445388793945\n",
            "step: 266, loss: 2.5818190574645996\n",
            "step: 267, loss: 2.5130255222320557\n",
            "step: 268, loss: 2.458794355392456\n",
            "step: 269, loss: 2.4482624530792236\n",
            "step: 270, loss: 2.4017555713653564\n",
            "step: 271, loss: 2.503430128097534\n",
            "step: 272, loss: 2.4009287357330322\n",
            "step: 273, loss: 2.3536384105682373\n",
            "step: 274, loss: 2.1556813716888428\n",
            "step: 275, loss: 2.379974126815796\n",
            "step: 276, loss: 2.0522892475128174\n",
            "step: 277, loss: 2.363189935684204\n",
            "step: 278, loss: 1.9450926780700684\n",
            "step: 279, loss: 2.049562931060791\n",
            "step: 280, loss: 2.3049914836883545\n",
            "step: 281, loss: 2.0163774490356445\n",
            "step: 282, loss: 2.25168514251709\n",
            "step: 283, loss: 2.0676026344299316\n",
            "step: 284, loss: 1.9666237831115723\n",
            "step: 285, loss: 2.0413641929626465\n",
            "step: 286, loss: 2.0227959156036377\n",
            "step: 287, loss: 1.7016639709472656\n",
            "step: 288, loss: 1.7809934616088867\n",
            "step: 289, loss: 1.8906506299972534\n",
            "step: 290, loss: 1.9699252843856812\n",
            "step: 291, loss: 1.9737553596496582\n",
            "step: 292, loss: 1.8847715854644775\n",
            "step: 293, loss: 1.7347877025604248\n",
            "step: 294, loss: 1.8907475471496582\n",
            "step: 295, loss: 1.6286042928695679\n",
            "step: 296, loss: 1.8405754566192627\n",
            "step: 297, loss: 1.8921840190887451\n",
            "step: 298, loss: 1.501820683479309\n",
            "step: 299, loss: 1.6098779439926147\n",
            "step: 300, loss: 1.8770312070846558\n",
            "step: 301, loss: 1.4263681173324585\n",
            "step: 302, loss: 1.7546303272247314\n",
            "step: 303, loss: 1.4867794513702393\n",
            "step: 304, loss: 1.5851205587387085\n",
            "step: 305, loss: 1.3284006118774414\n",
            "step: 306, loss: 1.5753295421600342\n",
            "step: 307, loss: 1.6192054748535156\n",
            "step: 308, loss: 1.4983752965927124\n",
            "step: 309, loss: 1.2665271759033203\n",
            "step: 310, loss: 1.415208339691162\n",
            "step: 311, loss: 1.677242398262024\n",
            "step: 312, loss: 1.3553470373153687\n",
            "step: 313, loss: 1.4777737855911255\n",
            "step: 314, loss: 1.5417323112487793\n",
            "step: 315, loss: 1.31431245803833\n",
            "step: 316, loss: 1.1930344104766846\n",
            "step: 317, loss: 1.376907229423523\n",
            "step: 318, loss: 1.4176335334777832\n",
            "step: 319, loss: 1.5280779600143433\n",
            "step: 320, loss: 1.2454458475112915\n",
            "step: 321, loss: 1.6538383960723877\n",
            "step: 322, loss: 1.453013300895691\n",
            "step: 323, loss: 1.3908268213272095\n",
            "step: 324, loss: 1.1080255508422852\n",
            "step: 325, loss: 1.2951046228408813\n",
            "step: 326, loss: 1.487144112586975\n",
            "step: 327, loss: 1.2382464408874512\n",
            "step: 328, loss: 0.9552471041679382\n",
            "step: 329, loss: 1.1076833009719849\n",
            "step: 330, loss: 1.3777257204055786\n",
            "step: 331, loss: 1.0822010040283203\n",
            "step: 332, loss: 1.115958333015442\n",
            "step: 333, loss: 1.1040074825286865\n",
            "step: 334, loss: 1.1069858074188232\n",
            "step: 335, loss: 0.9260499477386475\n",
            "step: 336, loss: 1.1435636281967163\n",
            "step: 337, loss: 1.3132805824279785\n",
            "step: 338, loss: 1.1836320161819458\n",
            "step: 339, loss: 1.1707878112792969\n",
            "step: 340, loss: 1.1102193593978882\n",
            "step: 341, loss: 1.1531076431274414\n",
            "step: 342, loss: 1.057007908821106\n",
            "step: 343, loss: 1.0977675914764404\n",
            "step: 344, loss: 1.0882600545883179\n",
            "step: 345, loss: 0.9812573194503784\n",
            "step: 346, loss: 0.7902793884277344\n",
            "step: 347, loss: 1.0838180780410767\n",
            "step: 348, loss: 1.1129781007766724\n",
            "step: 349, loss: 0.9052459001541138\n",
            "step: 350, loss: 1.3499716520309448\n",
            "step: 351, loss: 0.9336782693862915\n",
            "step: 352, loss: 1.0403430461883545\n",
            "step: 353, loss: 0.8640499711036682\n",
            "step: 354, loss: 0.9135888814926147\n",
            "step: 355, loss: 0.8614291548728943\n",
            "step: 356, loss: 0.9861907362937927\n",
            "step: 357, loss: 1.0898257493972778\n",
            "step: 358, loss: 0.9761877655982971\n",
            "step: 359, loss: 0.7937934398651123\n",
            "step: 360, loss: 0.9189790487289429\n",
            "step: 361, loss: 0.8142037391662598\n",
            "step: 362, loss: 0.9992431402206421\n",
            "step: 363, loss: 0.7265191078186035\n",
            "step: 364, loss: 1.0223557949066162\n",
            "step: 365, loss: 0.8434511423110962\n",
            "step: 366, loss: 0.809536874294281\n",
            "step: 367, loss: 0.9114190936088562\n",
            "step: 368, loss: 0.6607635617256165\n",
            "step: 369, loss: 0.8908805847167969\n",
            "step: 370, loss: 0.940104067325592\n",
            "step: 371, loss: 1.0058908462524414\n",
            "step: 372, loss: 1.0910488367080688\n",
            "step: 373, loss: 0.6482110619544983\n",
            "step: 374, loss: 0.999805748462677\n",
            "step: 375, loss: 1.08732271194458\n",
            "step: 376, loss: 0.7173333764076233\n",
            "step: 377, loss: 0.717552900314331\n",
            "step: 378, loss: 0.7997450232505798\n",
            "step: 379, loss: 0.7949324250221252\n",
            "step: 380, loss: 0.7279347777366638\n",
            "step: 381, loss: 0.8688928484916687\n",
            "step: 382, loss: 0.8980092406272888\n",
            "step: 383, loss: 0.7905056476593018\n",
            "step: 384, loss: 0.8873859643936157\n",
            "step: 385, loss: 0.8136143684387207\n",
            "step: 386, loss: 0.5262097716331482\n",
            "step: 387, loss: 0.6951370239257812\n",
            "step: 388, loss: 0.9067190289497375\n",
            "step: 389, loss: 0.6057561635971069\n",
            "step: 390, loss: 0.5612225532531738\n",
            "step: 391, loss: 0.7901109457015991\n",
            "step: 392, loss: 0.6428290605545044\n",
            "step: 393, loss: 0.572483241558075\n",
            "step: 394, loss: 0.3850449025630951\n",
            "step: 395, loss: 0.5874664783477783\n",
            "step: 396, loss: 1.0660731792449951\n",
            "step: 397, loss: 0.7351704239845276\n",
            "step: 398, loss: 0.6929734945297241\n",
            "step: 399, loss: 0.4769488573074341\n",
            "step: 400, loss: 0.6843234300613403\n",
            "step: 401, loss: 0.7493283152580261\n",
            "step: 402, loss: 0.5613893270492554\n",
            "step: 403, loss: 0.5795223712921143\n",
            "step: 404, loss: 0.5609642267227173\n",
            "step: 405, loss: 0.5306302905082703\n",
            "step: 406, loss: 0.6297493577003479\n",
            "step: 407, loss: 0.6472160220146179\n",
            "step: 408, loss: 0.6761480569839478\n",
            "step: 409, loss: 0.6685066223144531\n",
            "step: 410, loss: 0.48171767592430115\n",
            "step: 411, loss: 0.57384192943573\n",
            "step: 412, loss: 0.7554049491882324\n",
            "step: 413, loss: 0.5109414458274841\n",
            "step: 414, loss: 0.7862352728843689\n",
            "step: 415, loss: 0.5099740028381348\n",
            "step: 416, loss: 0.4884735941886902\n",
            "step: 417, loss: 0.6577755808830261\n",
            "step: 418, loss: 0.8247578144073486\n",
            "step: 419, loss: 0.8778504133224487\n",
            "step: 420, loss: 0.6478533148765564\n",
            "step: 421, loss: 0.6061349511146545\n",
            "step: 422, loss: 0.5678511261940002\n",
            "step: 423, loss: 0.6625543236732483\n",
            "step: 424, loss: 0.5642790794372559\n",
            "step: 425, loss: 0.5457512140274048\n",
            "step: 426, loss: 0.40536415576934814\n",
            "step: 427, loss: 0.5694986581802368\n",
            "step: 428, loss: 0.5939640998840332\n",
            "step: 429, loss: 0.523520827293396\n",
            "step: 430, loss: 0.4202338457107544\n",
            "step: 431, loss: 0.7230212688446045\n",
            "step: 432, loss: 0.39459818601608276\n",
            "step: 433, loss: 0.6516793966293335\n",
            "step: 434, loss: 0.6316907405853271\n",
            "step: 435, loss: 0.4220881760120392\n",
            "step: 436, loss: 0.48538458347320557\n",
            "step: 437, loss: 0.42251551151275635\n",
            "step: 438, loss: 0.5638148784637451\n",
            "step: 439, loss: 0.6159760355949402\n",
            "step: 440, loss: 0.6892337203025818\n",
            "step: 441, loss: 0.5868610143661499\n",
            "step: 442, loss: 0.5159507989883423\n",
            "step: 443, loss: 0.3778122663497925\n",
            "step: 444, loss: 0.3775685131549835\n",
            "step: 445, loss: 0.3448820114135742\n",
            "step: 446, loss: 0.35600295662879944\n",
            "step: 447, loss: 0.548626184463501\n",
            "step: 448, loss: 0.356923907995224\n",
            "step: 449, loss: 0.43967586755752563\n",
            "step: 450, loss: 0.5182996988296509\n",
            "step: 451, loss: 0.4776082932949066\n",
            "step: 452, loss: 0.4182748794555664\n",
            "step: 453, loss: 0.24909360706806183\n",
            "step: 454, loss: 0.5157120227813721\n",
            "step: 455, loss: 0.33928656578063965\n",
            "step: 456, loss: 0.233611062169075\n",
            "step: 457, loss: 0.23020030558109283\n",
            "step: 458, loss: 0.4495401382446289\n",
            "step: 459, loss: 0.2928997278213501\n",
            "step: 460, loss: 0.3604234755039215\n",
            "step: 461, loss: 0.26306065917015076\n",
            "step: 462, loss: 0.5427613854408264\n",
            "step: 463, loss: 0.1640147715806961\n",
            "step: 464, loss: 0.36733654141426086\n",
            "step: 465, loss: 0.24640542268753052\n",
            "step: 466, loss: 0.2838480472564697\n",
            "step: 467, loss: 0.3189168870449066\n",
            "step: 468, loss: 0.21571771800518036\n",
            "step: 469, loss: 0.2821013331413269\n",
            "step: 470, loss: 0.20972031354904175\n",
            "step: 471, loss: 0.25292760133743286\n",
            "step: 472, loss: 0.1716267615556717\n",
            "step: 473, loss: 0.2163948118686676\n",
            "step: 474, loss: 0.2275868058204651\n",
            "step: 475, loss: 0.2849884033203125\n",
            "step: 476, loss: 0.29124915599823\n",
            "step: 477, loss: 0.34713318943977356\n",
            "step: 478, loss: 0.30212700366973877\n",
            "step: 479, loss: 0.2439132183790207\n",
            "step: 480, loss: 0.2707470655441284\n",
            "step: 481, loss: 0.40760356187820435\n",
            "step: 482, loss: 0.2071087658405304\n",
            "step: 483, loss: 0.38820376992225647\n",
            "step: 484, loss: 0.24330581724643707\n",
            "step: 485, loss: 0.3267408609390259\n",
            "step: 486, loss: 0.2252671867609024\n",
            "step: 487, loss: 0.24444125592708588\n",
            "step: 488, loss: 0.2302384227514267\n",
            "step: 489, loss: 0.27252233028411865\n",
            "step: 490, loss: 0.12724973261356354\n",
            "step: 491, loss: 0.41388753056526184\n",
            "step: 492, loss: 0.241385817527771\n",
            "step: 493, loss: 0.2315557599067688\n",
            "step: 494, loss: 0.2626928389072418\n",
            "step: 495, loss: 0.239537313580513\n",
            "step: 496, loss: 0.23937354981899261\n",
            "step: 497, loss: 0.19452351331710815\n",
            "step: 498, loss: 0.20628730952739716\n",
            "step: 499, loss: 0.1461910903453827\n",
            "step: 500, loss: 0.15335974097251892\n",
            "step: 501, loss: 0.2636536657810211\n",
            "step: 502, loss: 0.24586457014083862\n",
            "step: 503, loss: 0.20898312330245972\n",
            "step: 504, loss: 0.1555051952600479\n",
            "step: 505, loss: 0.2211245745420456\n",
            "step: 506, loss: 0.3730810582637787\n",
            "step: 507, loss: 0.18405403196811676\n",
            "step: 508, loss: 0.273922324180603\n",
            "step: 509, loss: 0.30552974343299866\n",
            "step: 510, loss: 0.1386708766222\n",
            "step: 511, loss: 0.13030020892620087\n",
            "step: 512, loss: 0.1521504521369934\n",
            "step: 513, loss: 0.20358248054981232\n",
            "step: 514, loss: 0.1379486471414566\n",
            "step: 515, loss: 0.2835369110107422\n",
            "step: 516, loss: 0.22730512917041779\n",
            "step: 517, loss: 0.27737876772880554\n",
            "step: 518, loss: 0.3347998857498169\n",
            "step: 519, loss: 0.1971031129360199\n",
            "step: 520, loss: 0.1924332082271576\n",
            "step: 521, loss: 0.24710969626903534\n",
            "step: 522, loss: 0.2220536172389984\n",
            "step: 523, loss: 0.33182406425476074\n",
            "step: 524, loss: 0.3237279951572418\n",
            "step: 525, loss: 0.1576610505580902\n",
            "step: 526, loss: 0.2475786656141281\n",
            "step: 527, loss: 0.2441767454147339\n",
            "step: 528, loss: 0.28165203332901\n",
            "step: 529, loss: 0.13514673709869385\n",
            "step: 530, loss: 0.1376548409461975\n",
            "step: 531, loss: 0.1368817239999771\n",
            "step: 532, loss: 0.15303464233875275\n",
            "step: 533, loss: 0.31794238090515137\n",
            "step: 534, loss: 0.21502509713172913\n",
            "step: 535, loss: 0.2357310801744461\n",
            "step: 536, loss: 0.16442061960697174\n",
            "step: 537, loss: 0.18380595743656158\n",
            "step: 538, loss: 0.17300836741924286\n",
            "step: 539, loss: 0.15967807173728943\n",
            "step: 540, loss: 0.15971015393733978\n",
            "step: 541, loss: 0.23476535081863403\n",
            "step: 542, loss: 0.20882000029087067\n",
            "step: 543, loss: 0.39382675290107727\n",
            "step: 544, loss: 0.29156574606895447\n",
            "step: 545, loss: 0.14606212079524994\n",
            "step: 546, loss: 0.19198302924633026\n",
            "step: 547, loss: 0.1319916546344757\n",
            "step: 548, loss: 0.1344504952430725\n",
            "step: 549, loss: 0.2998846173286438\n",
            "step: 550, loss: 0.1645723581314087\n",
            "step: 551, loss: 0.2706361413002014\n",
            "step: 552, loss: 0.17601783573627472\n",
            "step: 553, loss: 0.35250237584114075\n",
            "step: 554, loss: 0.09313075989484787\n",
            "step: 555, loss: 0.20364071428775787\n",
            "step: 556, loss: 0.15469302237033844\n",
            "step: 557, loss: 0.14106348156929016\n",
            "step: 558, loss: 0.20439544320106506\n",
            "step: 559, loss: 0.17013098299503326\n",
            "step: 560, loss: 0.20951148867607117\n",
            "step: 561, loss: 0.2487107515335083\n",
            "step: 562, loss: 0.46543198823928833\n",
            "step: 563, loss: 0.1076987013220787\n",
            "step: 564, loss: 0.15028372406959534\n",
            "step: 565, loss: 0.1778162717819214\n",
            "step: 566, loss: 0.210616335272789\n",
            "step: 567, loss: 0.20574577152729034\n",
            "step: 568, loss: 0.21610425412654877\n",
            "step: 569, loss: 0.11810233443975449\n",
            "step: 570, loss: 0.2603691816329956\n",
            "step: 571, loss: 0.15595346689224243\n",
            "step: 572, loss: 0.23487751185894012\n",
            "step: 573, loss: 0.2043852061033249\n",
            "step: 574, loss: 0.09353549033403397\n",
            "step: 575, loss: 0.19499851763248444\n",
            "step: 576, loss: 0.1366071105003357\n",
            "step: 577, loss: 0.25893434882164\n",
            "step: 578, loss: 0.35585618019104004\n",
            "step: 579, loss: 0.25831860303878784\n",
            "step: 580, loss: 0.17992058396339417\n",
            "step: 581, loss: 0.30895304679870605\n",
            "step: 582, loss: 0.302091121673584\n",
            "step: 583, loss: 0.21246808767318726\n",
            "step: 584, loss: 0.17678742110729218\n",
            "step: 585, loss: 0.17543460428714752\n",
            "step: 586, loss: 0.20697149634361267\n",
            "step: 587, loss: 0.13988319039344788\n",
            "step: 588, loss: 0.08274757117033005\n",
            "step: 589, loss: 0.17372137308120728\n",
            "step: 590, loss: 0.17675156891345978\n",
            "step: 591, loss: 0.30373600125312805\n",
            "step: 592, loss: 0.14113551378250122\n",
            "step: 593, loss: 0.25766465067863464\n",
            "step: 594, loss: 0.301868736743927\n",
            "step: 595, loss: 0.282725065946579\n",
            "step: 596, loss: 0.1977841556072235\n",
            "step: 597, loss: 0.2095162272453308\n",
            "step: 598, loss: 0.23619061708450317\n",
            "step: 599, loss: 0.17417383193969727\n",
            "step: 600, loss: 0.18131911754608154\n",
            "step: 601, loss: 0.16088198125362396\n",
            "step: 602, loss: 0.2451842725276947\n",
            "step: 603, loss: 0.24692465364933014\n",
            "step: 604, loss: 0.2655719518661499\n",
            "step: 605, loss: 0.1288902312517166\n",
            "step: 606, loss: 0.10520053654909134\n",
            "step: 607, loss: 0.08538897335529327\n",
            "step: 608, loss: 0.2731228172779083\n",
            "step: 609, loss: 0.16569766402244568\n",
            "step: 610, loss: 0.1987469494342804\n",
            "step: 611, loss: 0.17159825563430786\n",
            "step: 612, loss: 0.02809673547744751\n",
            "step: 613, loss: 0.14690114557743073\n",
            "step: 614, loss: 0.06851904839277267\n",
            "step: 615, loss: 0.21610814332962036\n",
            "step: 616, loss: 0.1831752359867096\n",
            "step: 617, loss: 0.23235923051834106\n",
            "step: 618, loss: 0.2952859401702881\n",
            "step: 619, loss: 0.11757941544055939\n",
            "step: 620, loss: 0.1076987236738205\n",
            "step: 621, loss: 0.1324181705713272\n",
            "step: 622, loss: 0.2683151364326477\n",
            "step: 623, loss: 0.20450693368911743\n",
            "step: 624, loss: 0.21367046236991882\n",
            "step: 625, loss: 0.2360880821943283\n",
            "step: 626, loss: 0.13296547532081604\n",
            "step: 627, loss: 0.3274461328983307\n",
            "step: 628, loss: 0.18281935155391693\n",
            "step: 629, loss: 0.24812883138656616\n",
            "step: 630, loss: 0.2925930917263031\n",
            "step: 631, loss: 0.1853650063276291\n",
            "step: 632, loss: 0.10237547010183334\n",
            "step: 633, loss: 0.2444302886724472\n",
            "step: 634, loss: 0.15266305208206177\n",
            "step: 635, loss: 0.16748638451099396\n",
            "step: 636, loss: 0.20452088117599487\n",
            "step: 637, loss: 0.25258100032806396\n",
            "step: 638, loss: 0.10372214019298553\n",
            "step: 639, loss: 0.24821315705776215\n",
            "step: 640, loss: 0.22706325352191925\n",
            "step: 641, loss: 0.2840033769607544\n",
            "step: 642, loss: 0.2895776331424713\n",
            "step: 643, loss: 0.19492769241333008\n",
            "step: 644, loss: 0.12152453511953354\n",
            "step: 645, loss: 0.2096228152513504\n",
            "step: 646, loss: 0.22783546149730682\n",
            "step: 647, loss: 0.14154769480228424\n",
            "step: 648, loss: 0.10297798365354538\n",
            "step: 649, loss: 0.11706878989934921\n",
            "step: 650, loss: 0.10655418038368225\n",
            "step: 651, loss: 0.1542332023382187\n",
            "step: 652, loss: 0.1320611834526062\n",
            "step: 653, loss: 0.2711744010448456\n",
            "step: 654, loss: 0.3327537477016449\n",
            "step: 655, loss: 0.17371012270450592\n",
            "step: 656, loss: 0.10603530704975128\n",
            "step: 657, loss: 0.5517638921737671\n",
            "step: 658, loss: 0.13560780882835388\n",
            "step: 659, loss: 0.3781435787677765\n",
            "step: 660, loss: 0.27604344487190247\n",
            "step: 661, loss: 0.18594762682914734\n",
            "step: 662, loss: 0.25808459520339966\n",
            "step: 663, loss: 0.2491931915283203\n",
            "step: 664, loss: 0.32117801904678345\n",
            "step: 665, loss: 0.17323802411556244\n",
            "step: 666, loss: 0.06951701641082764\n",
            "step: 667, loss: 0.3095376193523407\n",
            "step: 668, loss: 0.2000972032546997\n",
            "step: 669, loss: 0.15667377412319183\n",
            "step: 670, loss: 0.21437948942184448\n",
            "step: 671, loss: 0.17796488106250763\n",
            "step: 672, loss: 0.09164131432771683\n",
            "step: 673, loss: 0.1926400512456894\n",
            "step: 674, loss: 0.25237399339675903\n",
            "step: 675, loss: 0.1789756715297699\n",
            "step: 676, loss: 0.16147223114967346\n",
            "step: 677, loss: 0.11869948357343674\n",
            "step: 678, loss: 0.08008147776126862\n",
            "step: 679, loss: 0.05098026618361473\n",
            "step: 680, loss: 0.06939525902271271\n",
            "step: 681, loss: 0.07854802161455154\n",
            "step: 682, loss: 0.03214389830827713\n",
            "step: 683, loss: 0.039995692670345306\n",
            "step: 684, loss: 0.038469478487968445\n",
            "step: 685, loss: 0.05338414013385773\n",
            "step: 686, loss: 0.034208741039037704\n",
            "step: 687, loss: 0.046342115849256516\n",
            "step: 688, loss: 0.04118726775050163\n",
            "step: 689, loss: 0.05220889672636986\n",
            "step: 690, loss: 0.040880147367715836\n",
            "step: 691, loss: 0.025043055415153503\n",
            "step: 692, loss: 0.07996693253517151\n",
            "step: 693, loss: 0.032348230481147766\n",
            "step: 694, loss: 0.04022664949297905\n",
            "step: 695, loss: 0.036919768899679184\n",
            "step: 696, loss: 0.11301743239164352\n",
            "step: 697, loss: 0.061141155660152435\n",
            "step: 698, loss: 0.037623222917318344\n",
            "step: 699, loss: 0.08713416755199432\n",
            "step: 700, loss: 0.045621585100889206\n",
            "step: 701, loss: 0.06690603494644165\n",
            "step: 702, loss: 0.028880605474114418\n",
            "step: 703, loss: 0.058950215578079224\n",
            "step: 704, loss: 0.038802508264780045\n",
            "step: 705, loss: 0.025721615180373192\n",
            "step: 706, loss: 0.022516701370477676\n",
            "step: 707, loss: 0.10477008670568466\n",
            "step: 708, loss: 0.03528884798288345\n",
            "step: 709, loss: 0.08434636145830154\n",
            "step: 710, loss: 0.017476482316851616\n",
            "step: 711, loss: 0.03472676873207092\n",
            "step: 712, loss: 0.05290365591645241\n",
            "step: 713, loss: 0.07038471102714539\n",
            "step: 714, loss: 0.03145585581660271\n",
            "step: 715, loss: 0.061391741037368774\n",
            "step: 716, loss: 0.021537862718105316\n",
            "step: 717, loss: 0.018329331651329994\n",
            "step: 718, loss: 0.03379463031888008\n",
            "step: 719, loss: 0.04138115793466568\n",
            "step: 720, loss: 0.03735806792974472\n",
            "step: 721, loss: 0.06246091052889824\n",
            "step: 722, loss: 0.03326188772916794\n",
            "step: 723, loss: 0.06068861484527588\n",
            "step: 724, loss: 0.06740690767765045\n",
            "step: 725, loss: 0.09293685853481293\n",
            "step: 726, loss: 0.12208502739667892\n",
            "step: 727, loss: 0.019746800884604454\n",
            "step: 728, loss: 0.026567377150058746\n",
            "step: 729, loss: 0.027071671560406685\n",
            "step: 730, loss: 0.033176444470882416\n",
            "step: 731, loss: 0.0960456058382988\n",
            "step: 732, loss: 0.043829530477523804\n",
            "step: 733, loss: 0.03212250769138336\n",
            "step: 734, loss: 0.01938316784799099\n",
            "step: 735, loss: 0.06558561325073242\n",
            "step: 736, loss: 0.025565333664417267\n",
            "step: 737, loss: 0.026129184290766716\n",
            "step: 738, loss: 0.034588318318128586\n",
            "step: 739, loss: 0.09070709347724915\n",
            "step: 740, loss: 0.04438134282827377\n",
            "step: 741, loss: 0.04086029902100563\n",
            "step: 742, loss: 0.043115053325891495\n",
            "step: 743, loss: 0.030461080372333527\n",
            "step: 744, loss: 0.06647682934999466\n",
            "step: 745, loss: 0.024290457367897034\n",
            "step: 746, loss: 0.027112120762467384\n",
            "step: 747, loss: 0.03563597425818443\n",
            "step: 748, loss: 0.0881720706820488\n",
            "step: 749, loss: 0.02709430269896984\n",
            "step: 750, loss: 0.046814121305942535\n",
            "step: 751, loss: 0.02348737232387066\n",
            "step: 752, loss: 0.04662974923849106\n",
            "step: 753, loss: 0.05874336138367653\n",
            "step: 754, loss: 0.06758052110671997\n",
            "step: 755, loss: 0.08073759078979492\n",
            "step: 756, loss: 0.027900967746973038\n",
            "step: 757, loss: 0.08226301521062851\n",
            "step: 758, loss: 0.03205018863081932\n",
            "step: 759, loss: 0.03604235500097275\n",
            "step: 760, loss: 0.024648213759064674\n",
            "step: 761, loss: 0.07648123055696487\n",
            "step: 762, loss: 0.04311605542898178\n",
            "step: 763, loss: 0.04430457204580307\n",
            "step: 764, loss: 0.04708335921168327\n",
            "step: 765, loss: 0.07213648408651352\n",
            "step: 766, loss: 0.049360841512680054\n",
            "step: 767, loss: 0.04462659731507301\n",
            "step: 768, loss: 0.05556046962738037\n",
            "step: 769, loss: 0.12019409239292145\n",
            "step: 770, loss: 0.046845048666000366\n",
            "step: 771, loss: 0.05283462628722191\n",
            "step: 772, loss: 0.08222105354070663\n",
            "step: 773, loss: 0.05275794118642807\n",
            "step: 774, loss: 0.01860373094677925\n",
            "step: 775, loss: 0.08400556445121765\n",
            "step: 776, loss: 0.04384534806013107\n",
            "step: 777, loss: 0.0845625102519989\n",
            "step: 778, loss: 0.04956898093223572\n",
            "step: 779, loss: 0.03296928107738495\n",
            "step: 780, loss: 0.04218270257115364\n",
            "step: 781, loss: 0.05150819197297096\n",
            "step: 782, loss: 0.05036957561969757\n",
            "step: 783, loss: 0.03894966095685959\n",
            "step: 784, loss: 0.06368852406740189\n",
            "step: 785, loss: 0.02079322561621666\n",
            "step: 786, loss: 0.10771716386079788\n",
            "step: 787, loss: 0.04348156973719597\n",
            "step: 788, loss: 0.080835722386837\n",
            "step: 789, loss: 0.037651024758815765\n",
            "step: 790, loss: 0.03336925432085991\n",
            "step: 791, loss: 0.04829855263233185\n",
            "step: 792, loss: 0.03818914294242859\n",
            "step: 793, loss: 0.04761189594864845\n",
            "step: 794, loss: 0.019146151840686798\n",
            "step: 795, loss: 0.07842040807008743\n",
            "step: 796, loss: 0.054249223321676254\n",
            "step: 797, loss: 0.034069232642650604\n",
            "step: 798, loss: 0.07043156772851944\n",
            "step: 799, loss: 0.05135340243577957\n",
            "step: 800, loss: 0.0779423862695694\n",
            "step: 801, loss: 0.024245668202638626\n",
            "step: 802, loss: 0.027283035218715668\n",
            "step: 803, loss: 0.056637052446603775\n",
            "step: 804, loss: 0.03192678466439247\n",
            "step: 805, loss: 0.025911487638950348\n",
            "step: 806, loss: 0.02078140340745449\n",
            "step: 807, loss: 0.10420133918523788\n",
            "step: 808, loss: 0.021223749965429306\n",
            "step: 809, loss: 0.0702022910118103\n",
            "step: 810, loss: 0.017393510788679123\n",
            "step: 811, loss: 0.05100532993674278\n",
            "step: 812, loss: 0.03849012404680252\n",
            "step: 813, loss: 0.03279818221926689\n",
            "step: 814, loss: 0.027200307697057724\n",
            "step: 815, loss: 0.046178217977285385\n",
            "step: 816, loss: 0.03153090551495552\n",
            "step: 817, loss: 0.057330578565597534\n",
            "step: 818, loss: 0.052158989012241364\n",
            "step: 819, loss: 0.033132147043943405\n",
            "step: 820, loss: 0.03451265022158623\n",
            "step: 821, loss: 0.015326869674026966\n",
            "step: 822, loss: 0.03036809153854847\n",
            "step: 823, loss: 0.07964219152927399\n",
            "step: 824, loss: 0.06489434093236923\n",
            "step: 825, loss: 0.019959039986133575\n",
            "step: 826, loss: 0.07933270186185837\n",
            "step: 827, loss: 0.02991419844329357\n",
            "step: 828, loss: 0.07467371225357056\n",
            "step: 829, loss: 0.04797971621155739\n",
            "step: 830, loss: 0.06132068485021591\n",
            "step: 831, loss: 0.06932969391345978\n",
            "step: 832, loss: 0.02236650139093399\n",
            "step: 833, loss: 0.029268188402056694\n",
            "step: 834, loss: 0.079093798995018\n",
            "step: 835, loss: 0.03684636577963829\n",
            "step: 836, loss: 0.061098337173461914\n",
            "step: 837, loss: 0.09489276260137558\n",
            "step: 838, loss: 0.031613871455192566\n",
            "step: 839, loss: 0.05469086766242981\n",
            "step: 840, loss: 0.03343953192234039\n",
            "step: 841, loss: 0.015083210542798042\n",
            "step: 842, loss: 0.06743124127388\n",
            "step: 843, loss: 0.03175981342792511\n",
            "step: 844, loss: 0.019449615851044655\n",
            "step: 845, loss: 0.028542371466755867\n",
            "step: 846, loss: 0.021751094609498978\n",
            "step: 847, loss: 0.02461942285299301\n",
            "step: 848, loss: 0.03262932226061821\n",
            "step: 849, loss: 0.07825049757957458\n",
            "step: 850, loss: 0.044954024255275726\n",
            "step: 851, loss: 0.029263019561767578\n",
            "step: 852, loss: 0.06452810019254684\n",
            "step: 853, loss: 0.0423920638859272\n",
            "step: 854, loss: 0.01591717265546322\n",
            "step: 855, loss: 0.009643319062888622\n",
            "step: 856, loss: 0.03311270847916603\n",
            "step: 857, loss: 0.02246040850877762\n",
            "step: 858, loss: 0.027056217193603516\n",
            "step: 859, loss: 0.06201506033539772\n",
            "step: 860, loss: 0.03789346665143967\n",
            "step: 861, loss: 0.014269095845520496\n",
            "step: 862, loss: 0.06718911230564117\n",
            "step: 863, loss: 0.034393955022096634\n",
            "step: 864, loss: 0.013642003759741783\n",
            "step: 865, loss: 0.02030680887401104\n",
            "step: 866, loss: 0.034153446555137634\n",
            "step: 867, loss: 0.040260061621665955\n",
            "step: 868, loss: 0.03944067656993866\n",
            "step: 869, loss: 0.06905955821275711\n",
            "step: 870, loss: 0.015100793913006783\n",
            "step: 871, loss: 0.033538445830345154\n",
            "step: 872, loss: 0.07738368958234787\n",
            "step: 873, loss: 0.04028928279876709\n",
            "step: 874, loss: 0.04318363964557648\n",
            "step: 875, loss: 0.016658244654536247\n",
            "step: 876, loss: 0.01158702839165926\n",
            "step: 877, loss: 0.042480021715164185\n",
            "step: 878, loss: 0.058708883821964264\n",
            "step: 879, loss: 0.03823864087462425\n",
            "step: 880, loss: 0.06063179671764374\n",
            "step: 881, loss: 0.03541461005806923\n",
            "step: 882, loss: 0.07022514939308167\n",
            "step: 883, loss: 0.049626126885414124\n",
            "step: 884, loss: 0.020351266488432884\n",
            "step: 885, loss: 0.020655862987041473\n",
            "step: 886, loss: 0.01885174959897995\n",
            "step: 887, loss: 0.034990038722753525\n",
            "step: 888, loss: 0.04228067770600319\n",
            "step: 889, loss: 0.011508920229971409\n",
            "step: 890, loss: 0.03277917206287384\n",
            "step: 891, loss: 0.026088882237672806\n",
            "step: 892, loss: 0.07118199020624161\n",
            "step: 893, loss: 0.021691059693694115\n",
            "step: 894, loss: 0.02534029632806778\n",
            "step: 895, loss: 0.02199399471282959\n",
            "step: 896, loss: 0.04672856628894806\n",
            "step: 897, loss: 0.01911148987710476\n",
            "step: 898, loss: 0.025607014074921608\n",
            "step: 899, loss: 0.024257216602563858\n",
            "step: 900, loss: 0.07121600210666656\n",
            "step: 901, loss: 0.05169862136244774\n",
            "step: 902, loss: 0.11040780693292618\n",
            "step: 903, loss: 0.04846782609820366\n",
            "step: 904, loss: 0.03317734971642494\n",
            "step: 905, loss: 0.013882518745958805\n",
            "step: 906, loss: 0.03200850635766983\n",
            "step: 907, loss: 0.00904155895113945\n",
            "step: 908, loss: 0.009350700303912163\n",
            "step: 909, loss: 0.028147166594862938\n",
            "step: 910, loss: 0.02044837176799774\n",
            "step: 911, loss: 0.011184727773070335\n",
            "step: 912, loss: 0.01799132116138935\n",
            "step: 913, loss: 0.01556145679205656\n",
            "step: 914, loss: 0.03700818121433258\n",
            "step: 915, loss: 0.010080642998218536\n",
            "step: 916, loss: 0.016215262934565544\n",
            "step: 917, loss: 0.01566801220178604\n",
            "step: 918, loss: 0.014956575818359852\n",
            "step: 919, loss: 0.014933454804122448\n",
            "step: 920, loss: 0.009508512914180756\n",
            "step: 921, loss: 0.023857854306697845\n",
            "step: 922, loss: 0.038804035633802414\n",
            "step: 923, loss: 0.05773315578699112\n",
            "step: 924, loss: 0.030857296660542488\n",
            "step: 925, loss: 0.0137687548995018\n",
            "step: 926, loss: 0.0125166866928339\n",
            "step: 927, loss: 0.01885030046105385\n",
            "step: 928, loss: 0.009821640327572823\n",
            "step: 929, loss: 0.03588007390499115\n",
            "step: 930, loss: 0.012845964170992374\n",
            "step: 931, loss: 0.02013792283833027\n",
            "step: 932, loss: 0.011566050350666046\n",
            "step: 933, loss: 0.04072880744934082\n",
            "step: 934, loss: 0.014864111319184303\n",
            "step: 935, loss: 0.014971581287682056\n",
            "step: 936, loss: 0.020236914977431297\n",
            "step: 937, loss: 0.04475602135062218\n",
            "step: 938, loss: 0.0543212853372097\n",
            "step: 939, loss: 0.011945366859436035\n",
            "step: 940, loss: 0.006893962621688843\n",
            "step: 941, loss: 0.02092519961297512\n",
            "step: 942, loss: 0.010342352092266083\n",
            "step: 943, loss: 0.019602075219154358\n",
            "step: 944, loss: 0.013525674119591713\n",
            "step: 945, loss: 0.012377610430121422\n",
            "step: 946, loss: 0.012000193819403648\n",
            "step: 947, loss: 0.010390330106019974\n",
            "step: 948, loss: 0.009321203455328941\n",
            "step: 949, loss: 0.017276456579566002\n",
            "step: 950, loss: 0.01505169365555048\n",
            "step: 951, loss: 0.008594155311584473\n",
            "step: 952, loss: 0.049154773354530334\n",
            "step: 953, loss: 0.004830884747207165\n",
            "step: 954, loss: 0.015372199937701225\n",
            "step: 955, loss: 0.038025904446840286\n",
            "step: 956, loss: 0.017097774893045425\n",
            "step: 957, loss: 0.010652519762516022\n",
            "step: 958, loss: 0.009835944510996342\n",
            "step: 959, loss: 0.014082717709243298\n",
            "step: 960, loss: 0.0117739113047719\n",
            "step: 961, loss: 0.016420895233750343\n",
            "step: 962, loss: 0.009348491206765175\n",
            "step: 963, loss: 0.011221055872738361\n",
            "step: 964, loss: 0.014276133850216866\n",
            "step: 965, loss: 0.008362457156181335\n",
            "step: 966, loss: 0.013308478519320488\n",
            "step: 967, loss: 0.03430585190653801\n",
            "step: 968, loss: 0.012037782929837704\n",
            "step: 969, loss: 0.009211258962750435\n",
            "step: 970, loss: 0.015315677970647812\n",
            "step: 971, loss: 0.012197344563901424\n",
            "step: 972, loss: 0.03641229122877121\n",
            "step: 973, loss: 0.007279615383595228\n",
            "step: 974, loss: 0.008743856102228165\n",
            "step: 975, loss: 0.009945455007255077\n",
            "step: 976, loss: 0.02005268819630146\n",
            "step: 977, loss: 0.011981137096881866\n",
            "step: 978, loss: 0.0213981531560421\n",
            "step: 979, loss: 0.0133458087220788\n",
            "step: 980, loss: 0.014903809875249863\n",
            "step: 981, loss: 0.012260918505489826\n",
            "step: 982, loss: 0.014652370475232601\n",
            "step: 983, loss: 0.010802102275192738\n",
            "step: 984, loss: 0.012362711131572723\n",
            "step: 985, loss: 0.01377758290618658\n",
            "step: 986, loss: 0.021714933216571808\n",
            "step: 987, loss: 0.013830465264618397\n",
            "step: 988, loss: 0.018144093453884125\n",
            "step: 989, loss: 0.014513600617647171\n",
            "step: 990, loss: 0.010156849399209023\n",
            "step: 991, loss: 0.014769431203603745\n",
            "step: 992, loss: 0.01074265968054533\n",
            "step: 993, loss: 0.021910393610596657\n",
            "step: 994, loss: 0.010050540789961815\n",
            "step: 995, loss: 0.012721321545541286\n",
            "step: 996, loss: 0.016954820603132248\n",
            "step: 997, loss: 0.005531551316380501\n",
            "step: 998, loss: 0.010637546889483929\n",
            "step: 999, loss: 0.010784244164824486\n",
            "step: 1000, loss: 0.010190083645284176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNlTANDhm5F4",
        "colab_type": "code",
        "outputId": "f645d871-1f44-4f43-f1de-fcf8be402cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#khôi phục các tham số đã lưu\n",
        "def restore_parameters(name, epoch):\n",
        "  filename= name.replace(':','-colon-')+ '-epoch-{}.txt'.format(epoch)\n",
        "  with open('/content/drive/My Drive/Python code/DS_Lab/Session 3 DS Lab/'+ filename) as f:\n",
        "  # with open('/content/drive/My Drive/'+ filename) as f:\n",
        "    line= f.read().splitlines()\n",
        "    if len(line)==1: # là 1 list\n",
        "      value= [float(number) for number in line[0].split(',')]\n",
        "    else:\n",
        "      value= [[float(number) for number in line[row].split(',')]\\\n",
        "              for row in range(len(line))]\n",
        "  return value\n",
        "  \n",
        "# đánh giá model trên test\n",
        "test_data_reader= DataReader(\n",
        "    # data_path= '/content/drive/My Drive/Python code/DS_Lab/test_tf_idf.txt',\n",
        "    data_path='/content/drive/My Drive/Python code/DS_Lab2/Data/test_tf_idf.txt',\n",
        "    batch_size=50,\n",
        "    vocab_size= vocab_size\n",
        ")\n",
        "mlp = MLP(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=50\n",
        "        )\n",
        "predicted_labels, loss = mlp.build_graph()\n",
        "with tf2.Session() as sess:\n",
        "  epoch=4\n",
        "\n",
        "  trainable_variables= tf2.trainable_variables()\n",
        "  for variable in trainable_variables:\n",
        "    saved_value= restore_parameters(variable.name, epoch)\n",
        "    assign_op= variable.assign(saved_value)\n",
        "    sess.run(assign_op)\n",
        "  \n",
        "  num_true_pred=0\n",
        "  while True:\n",
        "    test_data, test_labels= test_data_reader.next_batch()\n",
        "    test_plabel_eval= sess.run(\n",
        "        predicted_labels,\n",
        "        feed_dict={\n",
        "            mlp._x: test_data,\n",
        "            mlp._real_y: test_labels\n",
        "        }\n",
        "    )\n",
        "    matches= np.equal(test_plabel_eval, test_labels)\n",
        "    num_true_pred+= np.sum(matches.astype(float))\n",
        "\n",
        "    if test_data_reader._batch_id==0:\n",
        "      break\n",
        "  print(\"Epoch: \", epoch)\n",
        "  print(\"Accuracy on test data: \",num_true_pred/len(test_data_reader._data))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  4\n",
            "Accuracy on test data:  0.8035050451407328\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}